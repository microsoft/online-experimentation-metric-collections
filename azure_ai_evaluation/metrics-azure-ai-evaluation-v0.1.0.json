{
    "schemaVersion": "1.0",
    "metrics": [
        {
            "id": "evaluator_task_adherence",
            "displayName": "Task adherence score",
            "description": "Task adherence score given by Azure AI evaluation. The Task Adherence evaluator assesses how well an AI-generated response follows the assigned task in terms of alignment with instructions and definitions, accuracy and clarity, and proper use of provided tool definitions. Scoring is on a five-level integer scale: 0 – Fully Inadherent, 1 – Barely Adherent, 2 – Moderately Adherent, 3 – Mostly Adherent, and 4 – Fully Adherent. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.taskadherenceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'TaskAdherence'"
                }
            }
        },
        {
            "id": "evaluator_self_harm",
            "displayName": "Self-harm score",
            "description": "Self-harm score given by Azure AI evaluation. The evaluator analyzes a single turn or multi-turn conversation for content related to self-injury or suicide and returns an integer score from 0 (Very Low) to 7 (High). Higher values represent increasingly explicit, encouraging, or instructional self-harm content, while lower values cover purely informational mentions. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.selfharmevaluator?view=azure-python for details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'SelfHarm'"
                }
            }
        },
        {
            
        }
        {
            "id": "genai_evaluation_protected_material",
            "displayName": "Protected material score",
            "description": "Protected material score by Azure AI content safety API. The Protected material detection APIs scan the output of large language models to identify and flag known protected material. See (https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/protected-material?tabs=text) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'ProtectedMaterial'"
                }
            }
        },
        {
            "id": "genai_evaluation_hate_unfairness",
            "displayName": "Hate unfairness score",
            "description": "Hate and unfair score given by Azure AI evaluation. Hateful and unfair content refers to any language pertaining to hate toward or unfair representations of individuals and social groups along factors including but not limited to race, ethnicity, nationality, gender, sexual orientation, religion, immigration status, ability, personal appearance, and body size. Unfairness occurs when AI systems treat or represent social groups inequitably, creating or contributing to societal inequities. Safety evaluations annotate self-harm-related content using a 0-7 scale. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.hateunfairnessevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'HateUnfairness'"
                }
            }
        },
        {
            "id": "genai_evaluation_sexual",
            "displayName": "Sexual content score",
            "description": "Score for sexual content given by Azure AI evaluation. Sexual score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.sexualevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Sexual'"
                }
            }
        },
        {
            "id": "genai_evaluation_violence",
            "displayName": "Violent content score",
            "description": "Violence score given by Azure AI evaluation, Violence score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.violenceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Violence'"
                }
            }
        },
        {
            "id": "genai_evaluation_relevance",
            "displayName": "Relevance score",
            "description": "Relevance score given by Azure AI evaluation. The relevance measure assesses the ability of answers to capture the key points of the context. High relevance scores signify the AI system's understanding of the input and its capability to produce coherent and contextually appropriate outputs. Conversely, low relevance scores indicate that generated responses might be off-topic, lacking in context, or insufficient in addressing the user's intended queries. Relevance scores range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.relevanceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Relevance'"
                }
            }
        },
        {
            "id": "genai_evaluation_fluency",
            "displayName": "Fluency score",
            "description": "Fluency score given by Azure AI evaluation. The fluency measure assesses the extent to which the generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage, resulting in linguistically correct responses. The fluency score range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.fluencyevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Fluency'"
                }
            }
        },
        {
            "id": "genai_evaluation_coherence",
            "displayName": "Coherence score",
            "description": "Coherence score given by Azure AI evaluation. The coherence measure assesses the ability of the language model to generate text that reads naturally, flows smoothly, and resembles human-like language in its responses. Use it when assessing the readability and user-friendliness of a model's generated responses in real-world applications. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.coherenceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Coherence'"
                }
            }
        },
        {
            "id": "genai_evaluation_retrieval",
            "displayName": "Coherence score",
            "description": "Retrieval score given by Azure AI evaluation. The retrieval evaluator measures how effectively an AI system selects and ranks the most relevant information for a query or multi-turn conversation (e.g., in RAG scenarios). Scores range from 1 (worst) to 5 (best). High scores indicate that the system surfaces the top-ranked, contextually relevant chunks without introducing bias from external knowledge, whereas low scores suggest poor ranking and/or biased, fact-insensitive retrieval. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.retrievalevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Retrieval'"
                }
            }
        },
        {
            "id": "genai_evaluation_groundedness",
            "displayName": "Groundedness score",
            "description": "Groundedness score given by Azure AI evaluation. This evaluator checks how well each claim in the model’s response is substantiated by the supplied context; factually correct but unsupported statements are marked ungrounded. Use this metric to verify that answers align with and are validated by the provided sources. Scores range from 1 (least grounded) to 5 (most grounded). See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.groundednessproevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "tags": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "kind": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Groundedness'"
                }
            }
        }
    ]
}