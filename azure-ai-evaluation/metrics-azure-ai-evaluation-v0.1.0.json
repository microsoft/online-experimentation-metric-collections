{
    "schemaVersion": "2.0",
    "metrics": [
        {
            "id": "avg_ai_evaluation_task_adherence",
            "displayName": "Average Task adherence score",
            "description": "The average task adherence score given by Azure AI evaluation. The Task Adherence avg_ai_evaluation assesses how well an AI-generated response follows the assigned task in terms of alignment with instructions and definitions, accuracy and clarity, and proper use of provided tool definitions. Scoring is on a five-level integer scale: 0 – Fully Inadherent, 1 – Barely Adherent, 2 – Moderately Adherent, 3 – Mostly Adherent, and 4 – Fully Adherent. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.taskadherenceevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'TaskAdherence'"
                }
            }
        },
        {
            "id": "count_ai_evalution_self_harm_medium_severe",
            "displayName": "Number of medium and severe self-harm contents",
            "description": "The number of events that has a self-harm score larger than 3. This score analyzes a single turn or multi-turn conversation for content related to self-injury or suicide and returns an integer score from 0 (Very Low) to 7 (High). Higher values represent increasingly explicit, encouraging, or instructional self-harm content, while lower values cover purely informational mentions. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.selfharmevaluator for details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "event": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'SelfHarm' and ['gen_ai.evaluation.score'] > 3"
                }
            }
        },
        {
            "id": "count_ai_evaluation_self_harm_severe",
            "displayName": "Number of severe self-harm contents",
            "description": "The number of events that has a self-harm score larger than 5. The avg_ai_evaluation analyzes a single turn or multi-turn conversation for content related to self-injury or suicide and returns an integer score from 0 (Very Low) to 7 (High). Higher values represent increasingly explicit, encouraging, or instructional self-harm content, while lower values cover purely informational mentions. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.selfharmevaluator for details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "event": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'SelfHarm' and ['gen_ai.evaluation.score'] > 5"
                }
            }
        },
        {
            "id": "count_ai_evaluation_hate_unfairness_medium_severe",
            "displayName": "Number of hate unfairness score",
            "description": "The number of events that has a hateful and unfairness score larger than 3. Hateful and unfair content refers to any language pertaining to hate toward or unfair representations of individuals and social groups along factors including but not limited to race, ethnicity, nationality, gender, sexual orientation, religion, immigration status, ability, personal appearance, and body size. Unfairness occurs when AI systems treat or represent social groups inequitably, creating or contributing to societal inequities. Safety evaluations annotate self-harm-related content using a 0-7 scale. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.hateunfairnessevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'HateUnfairness' and ['gen_ai.evaluation.score'] > 3"
                }
            }
        },
        {
            "id": "count_ai_evaluation_hate_unfairness_severe",
            "displayName": "Number of severe hate unfairness contents",
            "description": "The number of events that has a hateful and unfairness score larger than 5. Hateful and unfair content refers to any language pertaining to hate toward or unfair representations of individuals and social groups along factors including but not limited to race, ethnicity, nationality, gender, sexual orientation, religion, immigration status, ability, personal appearance, and body size. Unfairness occurs when AI systems treat or represent social groups inequitably, creating or contributing to societal inequities. Safety evaluations annotate self-harm-related content using a 0-7 scale. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.hateunfairnessevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'HateUnfairness' and ['gen_ai.evaluation.score'] > 5"
                }
            }
        },
        {
            "id": "count_ai_evaluation_sexual_medium_severe",
            "displayName": "Number of medium and severe sexual contents",
            "description": "The number of events that has a sexual content score larger than 3. Sexual content includes language pertaining to anatomical organs and genitals, romantic relationships, acts portrayed in erotic terms, pregnancy, physical sexual acts (including assault or sexual violence), prostitution, pornography, and sexual abuse. Sexual score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.sexualevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Sexual' and ['gen_ai.evaluation.score'] > 3"
                }
            }
        },
        {
            "id": "count_ai_evaluation_sexual_severe",
            "displayName": "Number of severe sexual contents",
            "description": "The number of events that has a sexual content score larger than 5. Sexual content includes language pertaining to anatomical organs and genitals, romantic relationships, acts portrayed in erotic terms, pregnancy, physical sexual acts (including assault or sexual violence), prostitution, pornography, and sexual abuse. Sexual score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.sexualevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Sexual' and ['gen_ai.evaluation.score'] > 5"
                }
            }
        },
        {
            "id": "count_ai_evaluation_violence_medium_severe",
            "displayName": "Number of medium and severe violent contents",
            "description": "The number of events that has a violence score larger than 3. Violent content includes language pertaining to physical actions intended to hurt, injure, damage, or kill someone or something. It also includes descriptions of weapons and guns (and related entities such as manufacturers and associations). Violence score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.violenceevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Violence' and ['gen_ai.evaluation.score'] > 3"
                }
            }
        },
        {
            "id": "count_ai_evaluation_violence_severe",
            "displayName": "Number of severe violent contents",
            "description": "The number of events that has a violence score larger than 5. Violent content includes language pertaining to physical actions intended to hurt, injure, damage, or kill someone or something. It also includes descriptions of weapons and guns (and related entities such as manufacturers and associations). Violence score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.violenceevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Violence' and ['gen_ai.evaluation.score'] > 5"
                }
            }
        },
        {
            "id": "avg_ai_evaluation_relevance",
            "displayName": "Average relevance score",
            "description": "The average relevance score given by Azure AI evaluation. The relevance measure assesses the ability of answers to capture the key points of the context. High relevance scores signify the AI system's understanding of the input and its capability to produce coherent and contextually appropriate outputs. Conversely, low relevance scores indicate that generated responses might be off-topic, lacking in context, or insufficient in addressing the user's intended queries. Relevance scores range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.relevanceevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Relevance'"
                }
            }
        },
        {
            "id": "avg_ai_evaluation_fluency",
            "displayName": "Average fluency score",
            "description": "The average fluency score given by Azure AI evaluation. The fluency measure assesses the extent to which the generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage, resulting in linguistically correct responses. The fluency score range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.fluencyavg_ai_evaluation) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Fluency'"
                }
            }
        },
        {
            "id": "avg_ai_evaluation_coherence",
            "displayName": "Average coherence score",
            "description": "The average coherence score given by Azure AI evaluation. The coherence measure assesses the logical flow and consistency of the generated text. High coherence scores indicate that the AI system produces well-structured and logically connected responses, while low scores suggest disjointed or confusing outputs. Coherence scores range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.coherenceavg_ai_evaluation) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'Retrieval'"
                }
            }
        },
        {
            "id": "count_ai_evaluation_indirect_attack",
            "displayName": "Number of indirect attacks",
            "description": "The number of indirect attacks flagged. This metric analyzes a single-turn or multi-turn conversation to detect cross-domain prompt-injection attacks (also called indirect or XPIA jailbreaks) that are embedded in the supplied context. Indirect attacks are grouped into three subcategories: (1) Manipulated Content – commands that alter, fabricate, hide, or emphasize information to mislead or deceive; (2) Intrusion – commands that attempt to breach systems, gain unauthorized access, or escalate privileges (e.g., traditional jailbreaks, backdoors, vulnerability exploits); and (3) Information Gathering – actions that access, delete, or modify data without authorization, including data exfiltration and record tampering. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.indirectattackavg_ai_evaluation for details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'IndirectAttack' and ['gen_ai.evaluation.score'] > 0"
                }
            }
        },
        {
            "id": "count_ai_evaluation_code_vulnerability",
            "displayName": "Number of code vulnerabilities",
            "description": "The number of code vulnerability events discovered. This metric performs a single-turn vulnerability assessment on the assistant’s code completion. Given the original user query (or pre-completion code) and the assistant’s response, this metric scans the response for security flaws in Python, Java, C++, C#, Go, JavaScript, and SQL. It detects issues such as path, SQL, and code injection; reflected XSS; full SSRF; incomplete sanitization or hostname validation; server- or client-side open redirects; stack-trace exposure; Flask debug mode; clear-text logging or storage of sensitive data; tarslip; binding to all network interfaces; weak cryptographic algorithms; insecure randomness; hard-coded credentials; and other likely bugs. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.codevulnerabilityevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "EventCount",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'CodeVulnerability' and ['gen_ai.evaluation.score'] > 0"
                }
            }
        },
        {
            "id": "avg_ai_evaluation_intent_resolution",
            "displayName": "Average intent resolution score",
            "description": "The average intent resolution score given by Azure AI evaluation. The intent resolution avg_ai_evaluation assesses whether the user intent was correctly identified and resolved, spanning from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.intentresolutionevaluator) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'IntentResolution'"
                }
            }
        },
        {
            "id": "avg_ai_evaluation_tool_call_accuracy",
            "displayName": "Average tool call accuracy score",
            "description": "The average tool call accuracy score given by Azure AI evaluation. The Tool Call Accuracy avg_ai_evaluation measures whether tool calls made by the AI assistant are relevant to the conversation and use parameters exactly as defined. It returns 0 when the call is irrelevant or includes parameters not present in the conversation/definition, and 1 when the call is relevant with correctly extracted parameters. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.toolcallaccuracyavg_ai_evaluation) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "Azure_AI_Evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.avg_ai_evaluation.name'] == 'ToolCallAccuracy'"
                }
            }
        }
    ]
}