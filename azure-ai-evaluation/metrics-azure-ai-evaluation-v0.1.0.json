{
    "schemaVersion": "2.0",
    "metrics": [
        {
            "id": "evaluator_task_adherence",
            "displayName": "Task adherence score",
            "description": "Task adherence score given by Azure AI evaluation. The Task Adherence evaluator assesses how well an AI-generated response follows the assigned task in terms of alignment with instructions and definitions, accuracy and clarity, and proper use of provided tool definitions. Scoring is on a five-level integer scale: 0 – Fully Inadherent, 1 – Barely Adherent, 2 – Moderately Adherent, 3 – Mostly Adherent, and 4 – Fully Adherent. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.taskadherenceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'TaskAdherence'"
                }
            }
        },
        {
            "id": "evaluator_self_harm",
            "displayName": "Self-harm score",
            "description": "Self-harm score given by Azure AI evaluation. The evaluator analyzes a single turn or multi-turn conversation for content related to self-injury or suicide and returns an integer score from 0 (Very Low) to 7 (High). Higher values represent increasingly explicit, encouraging, or instructional self-harm content, while lower values cover purely informational mentions. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.selfharmevaluator?view=azure-python for details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'SelfHarm'"
                }
            }
        },
        {
            "id": "genai_evaluation_hate_unfairness",
            "displayName": "Hate unfairness score",
            "description": "Hate and unfair score given by Azure AI evaluation. Hateful and unfair content refers to any language pertaining to hate toward or unfair representations of individuals and social groups along factors including but not limited to race, ethnicity, nationality, gender, sexual orientation, religion, immigration status, ability, personal appearance, and body size. Unfairness occurs when AI systems treat or represent social groups inequitably, creating or contributing to societal inequities. Safety evaluations annotate self-harm-related content using a 0-7 scale. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.hateunfairnessevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'HateUnfairness'"
                }
            }
        },
        {
            "id": "genai_evaluation_sexual",
            "displayName": "Sexual content score",
            "description": "Score for sexual content given by Azure AI evaluation. Sexual score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.sexualevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Sexual'"
                }
            }
        },
        {
            "id": "genai_evaluation_violence",
            "displayName": "Violent content score",
            "description": "Violence score given by Azure AI evaluation, Violence score is range from 0 to 7. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.violenceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Violence'"
                }
            }
        },
        {
            "id": "genai_evaluation_relevance",
            "displayName": "Relevance score",
            "description": "Relevance score given by Azure AI evaluation. The relevance measure assesses the ability of answers to capture the key points of the context. High relevance scores signify the AI system's understanding of the input and its capability to produce coherent and contextually appropriate outputs. Conversely, low relevance scores indicate that generated responses might be off-topic, lacking in context, or insufficient in addressing the user's intended queries. Relevance scores range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.relevanceevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Relevance'"
                }
            }
        },
        {
            "id": "genai_evaluation_fluency",
            "displayName": "Fluency score",
            "description": "Fluency score given by Azure AI evaluation. The fluency measure assesses the extent to which the generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage, resulting in linguistically correct responses. The fluency score range from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.fluencyevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Fluency'"
                }
            }
        },
        {
            "id": "genai_evaluation_retrieval",
            "displayName": "Coherence score",
            "description": "Retrieval score given by Azure AI evaluation. The retrieval evaluator measures how effectively an AI system selects and ranks the most relevant information for a query or multi-turn conversation (e.g., in RAG scenarios). Scores range from 1 (worst) to 5 (best). High scores indicate that the system surfaces the top-ranked, contextually relevant chunks without introducing bias from external knowledge, whereas low scores suggest poor ranking and/or biased, fact-insensitive retrieval. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.retrievalevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'Retrieval'"
                }
            }
        },
        {
            "id": "genai_evaluation_indirect_attack",
            "displayName": "Indirect attack score",
            "description": "Indirect attack score given by Azure AI evaluation. This evaluator analyzes a single-turn or multi-turn conversation to detect cross-domain prompt-injection attacks (also called indirect or XPIA jailbreaks) that are embedded in the supplied context. Indirect attacks are grouped into three subcategories: (1) Manipulated Content – commands that alter, fabricate, hide, or emphasize information to mislead or deceive; (2) Intrusion – commands that attempt to breach systems, gain unauthorized access, or escalate privileges (e.g., traditional jailbreaks, backdoors, vulnerability exploits); and (3) Information Gathering – actions that access, delete, or modify data without authorization, including data exfiltration and record tampering. The evaluator returns a boolean value where true indicates that the response contains an indirect attack. See https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.indirectattackevaluator?view=azure-python for details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'IndirectAttack'"
                }
            }
        },
        {
            "id": "genai_evaluation_code_vulnerability",
            "displayName": "Code vulnerability score",
            "description": "Performs a single-turn vulnerability assessment on the assistant’s code completion. Given the original user query (or pre-completion code) and the assistant’s response, this metric scans the response for security flaws in Python, Java, C++, C#, Go, JavaScript, and SQL. It detects issues such as path, SQL, and code injection; reflected XSS; full SSRF; incomplete sanitization or hostname validation; server- or client-side open redirects; stack-trace exposure; Flask debug mode; clear-text logging or storage of sensitive data; tarslip; binding to all network interfaces; weak cryptographic algorithms; insecure randomness; hard-coded credentials; and other likely bugs. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.codevulnerabilityevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Decrease",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'CodeVulnerability'"
                }
            }
        },
        {
            "id": "genai_evaluation_intent_resolution",
            "displayName": "Intent resolution score",
            "description": "The intent resolution evaluator assesses whether the user intent was correctly identified and resolved, spanning from 1 to 5. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.intentresolutionevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'IntentResolution'"
                }
            }
        },
        {
            "id": "genai_evaluation_tool_call_accuracy",
            "displayName": "Tool call accuracy score",
            "description": "Tool call accuracy score given by Azure AI evaluation. The Tool Call Accuracy evaluator measures whether tool calls made by the AI assistant are relevant to the conversation and use parameters exactly as defined. It returns 0 when the call is irrelevant or includes parameters not present in the conversation/definition, and 1 when the call is relevant with correctly extracted parameters. See (https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.toolcallaccuracyevaluator?view=azure-python) for more details.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'ToolCallAccuracy'"
                }
            }
        },
        {
            "id": "genai_evaluation_openai_string_check_grader",
            "displayName": "OpenAI string check grader score",
            "description": "String-check grader configuration. Compares the provided \"input\" with the \"reference\" string using one of four operations — eq, ne, like, or ilike — and returns 1 for a match according to the chosen rule (case-sensitive for eq, ne, like; case-insensitive for ilike), otherwise 0. Ideal for simple pass/fail evaluations such as exact answers, yes/no responses, or answers that must contain specific text.",
            "lifecycle": "Active",
            "categories": [
                "GenAI",
                "azure_ai_evaluation"
            ],
            "desiredDirection": "Increase",
            "definition": {
                "type": "Average",
                "value": {
                    "eventName": "gen_ai.evaluation.result",
                    "eventProperty": "gen_ai.evaluation.score",
                    "filter": "['gen_ai.evaluator.name'] == 'OpenAIStringCheckGrader'"
                }
            }
        }
    ]
}